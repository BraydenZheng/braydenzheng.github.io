{"posts":[{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2023/01/22/hello-world/"},{"title":"Data Prepartion","text":"1. Gather data from API12import requestsimport pandas as pd Using the amazon price api from rapidapi.com 1url = &quot;https://amazon-price1.p.rapidapi.com/search&quot; Here, I am querying a bottom bracket used for replacing on my bicycle 12345678910querystring = {&quot;keywords&quot;:&quot;bottom bracket&quot;,&quot;marketplace&quot;:&quot;ES&quot;}headers = { &quot;X-RapidAPI-Key&quot;: &quot;hidden here&quot;, &quot;X-RapidAPI-Host&quot;: &quot;amazon-price1.p.rapidapi.com&quot;}response = requests.request(&quot;GET&quot;, url, headers=headers, params=querystring)print(response.text) Store the result in panda format 1df = pd.read_json(response.text) 1df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ASIN title price listPrice imageUrl detailPageURL rating totalReviews subtitle isPrimeEligible 0 B005L83TF4 SUN RACE BBS15 Bottom Bracket 68/127MM-STEEL E... 13,14 € https://m.media-amazon.com/images/I/316upP-m6I... https://www.amazon.es/dp/B005L83TF4 3.8 33 0 1 B006RM70JY The Bottom Bracket (English Edition) 2,99 € https://m.media-amazon.com/images/I/411tM3tvfN... https://www.amazon.es/dp/B006RM70JY 4.5 2 0 2 B075GQJFL1 Bottom Bracket 0,99 € https://m.media-amazon.com/images/I/71jDB9f3AK... https://www.amazon.es/dp/B075GQJFL1 0 3 B075DW91K2 Bottom Bracket 0,99 € https://m.media-amazon.com/images/I/71Tr4XpGvd... https://www.amazon.es/dp/B075DW91K2 0 4 B09D8S31V7 Bottom Bracket Racket 1,29 € https://m.media-amazon.com/images/I/41qG7oY5Ky... https://www.amazon.es/dp/B09D8S31V7 0 5 B08K2WHFS2 Dandy in the Underworld 1,29 € https://m.media-amazon.com/images/I/51UcKxQdUn... https://www.amazon.es/dp/B08K2WHFS2 0 6 B08K2VYJFK Dandy in the Underworld 1,29 € https://m.media-amazon.com/images/I/51UcKxQdUn... https://www.amazon.es/dp/B08K2VYJFK 0 7 B01J5083XQ Solstice 0,99 € https://m.media-amazon.com/images/I/51avvd1duQ... https://www.amazon.es/dp/B01J5083XQ 0 8 B01J5082CS Solstice 0,99 € https://m.media-amazon.com/images/I/51avvd1duQ... https://www.amazon.es/dp/B01J5082CS 0 9 B09J781T8V Soporte Inferior, Bottom Bracket, Token Ninja ... 74,01 € https://m.media-amazon.com/images/I/312Gc5vg36... https://www.amazon.es/dp/B09J781T8V 5.0 1 0 2. Gather data from website2.1 download from academic websiteDr. Ni’s website contains a varitey of amazon products reviews / metadata avaialbe for research usage. Download the metadata from electronics category 1curl -O https://drive.google.com/file/d/1QKGvsVNDfHJORr_3atR69B06UuHFJzK9/view?usp=share_link CitationJustifying recommendations using distantly-labeled reviews and fined-grained aspectsJianmo Ni, Jiacheng Li, Julian McAuleyEmpirical Methods in Natural Language Processing (EMNLP), 2019 2.2 download from Kaggle1234567import osimport jsonimport pandas as pdimport numpy as npimport dataframe_image as dfiimport seaborn as snsimport matplotlib.pyplot as plt Loading the data downloaded from kaggle 1df = pd.read_csv('amazon_fine_food/Reviews.csv') 1df.head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Id ProductId UserId ProfileName HelpfulnessNumerator HelpfulnessDenominator Score Time Summary Text 0 1 B001E4KFG0 A3SGXH7AUHU8GW delmartian 1 1 5 1303862400 Good Quality Dog Food I have bought several of the Vitality canned d... 1 2 B00813GRG4 A1D87F6ZCVE5NK dll pa 0 0 1 1346976000 Not as Advertised Product arrived labeled as Jumbo Salted Peanut... 2 3 B000LQOCH0 ABXLMWJIXXAIN Natalia Corres \"Natalia Corres\" 1 1 4 1219017600 \"Delight\" says it all This is a confection that has been around a fe... 3 4 B000UA0QIQ A395BORC6FGVXV Karl 3 3 2 1307923200 Cough Medicine If you are looking for the secret ingredient i... 4 5 B006K2ZZ7K A1UQRSCLF8GW1T Michael D. Bigham \"M. Wassir\" 0 0 5 1350777600 Great taffy Great taffy at a great price. There was a wid... Export raw data as image 1dfi.export(df.head(10), 'img/raw_data.png') 3. Data cleaning and visualize3.1 Clean irrelevant data columnUserId, profileName, Timestamp is not related to product recommendation, so it’s better to remove them off 1df = df.drop('UserId', axis = 1) 1df = df.drop('ProfileName', axis = 1) 1df = df.drop('Time', axis = 1) 1df.head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Id ProductId HelpfulnessNumerator HelpfulnessDenominator Score Summary Text 0 1 B001E4KFG0 1 1 5 Good Quality Dog Food I have bought several of the Vitality canned d... 1 2 B00813GRG4 0 0 1 Not as Advertised Product arrived labeled as Jumbo Salted Peanut... 2 3 B000LQOCH0 1 1 4 \"Delight\" says it all This is a confection that has been around a fe... 3 4 B000UA0QIQ 3 3 2 Cough Medicine If you are looking for the secret ingredient i... 4 5 B006K2ZZ7K 0 0 5 Great taffy Great taffy at a great price. There was a wid... 3.2 Inspect Duplicate Data1productId_c = df.iloc[:,1:2] 1productId_c.head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ProductId count 0 B001E4KFG0 0 1 B00813GRG4 0 2 B000LQOCH0 0 3 B000UA0QIQ 0 4 B006K2ZZ7K 0 1productId_c.insert(1, 'count',0) Group by prouduct id to count num of duplicated for each id 1p_f = productId_c.groupby(['ProductId']).transform('count') 1p_f.sort_values(by=['count'], ascending=False).head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } count frequency 563881 913 913 563615 913 913 563629 913 913 563628 913 913 563627 913 913 1p_f['frequency'] = 0 1p_f.head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } count frequency 0 1 30408 1 1 30408 2 1 30408 3 1 30408 4 4 17296 1p_a = p_f.groupby(['count']).count() Group by count got early to calculate frequency for each duplicate number 1p_a['frequency'] = p_f.groupby(['count']).transform('count') 1p_a.iloc[:,0] count 1 30408 2 24524 3 20547 4 17296 5 15525 ... 564 5076 567 567 623 623 632 2528 913 913 Name: frequency, Length: 283, dtype: int64 1p_a.index Int64Index([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, ... 491, 506, 530, 542, 556, 564, 567, 623, 632, 913], dtype='int64', name='count', length=283) 1p_a.head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } frequency count 1 30408 2 24524 3 20547 4 17296 5 15525 Build an image showing the num of duplcated product existed in data set, it’s tremendous 123456789# Horizontal Bar Plot show duplicated count - num of productsplt.bar(p_a.index, p_a.iloc[0])plt.xlabel(&quot;duplicated count&quot;)plt.ylabel(&quot;num of products&quot;)plt.title(&quot;duplicated products ditribution&quot;)plt.savefig('img/duplicated_product_distribution.png')# Show Plotplt.show() 3.3 Merge Duplicated DataThe main focus here is merging duplicated data, making each product id unique For those product contains multiple scores, counting an average probably be a good choice for it. The drawback is some unique fields also need to be removed (‘Text’, ‘Summary’). Since the score plays a determined factor in recommendation, removing is necessary for certain analysis. 1df_avg = df.iloc[:, 1:5] 1df_avg .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ProductId HelpfulnessNumerator HelpfulnessDenominator Score 0 B001E4KFG0 1 1 5 1 B00813GRG4 0 0 1 2 B000LQOCH0 1 1 4 3 B000UA0QIQ 3 3 2 4 B006K2ZZ7K 0 0 5 ... ... ... ... ... 568449 B001EO7N10 0 0 5 568450 B003S1WTCU 0 0 2 568451 B004I613EE 2 2 5 568452 B004I613EE 1 1 5 568453 B001LR2CU2 0 0 5 568454 rows × 4 columns 1avg = df_avg.groupby(['ProductId']).mean() 1avg.insert(0, 'ProductId', avg.index) 1avg.index = range(len(avg.index)) Cleaned Data generated, duplicated value eliminated 1avg .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ProductId HelpfulnessNumerator HelpfulnessDenominator Score 0 0006641040 3.027027 3.378378 4.351351 1 141278509X 1.000000 1.000000 5.000000 2 2734888454 0.500000 0.500000 3.500000 3 2841233731 0.000000 0.000000 5.000000 4 7310172001 0.809249 1.219653 4.751445 ... ... ... ... ... 74253 B009UOFTUI 0.000000 0.000000 1.000000 74254 B009UOFU20 0.000000 0.000000 1.000000 74255 B009UUS05I 0.000000 0.000000 5.000000 74256 B009WSNWC4 0.000000 0.000000 5.000000 74257 B009WVB40S 0.000000 0.000000 5.000000 74258 rows × 4 columns 1avg['HelpfulRatio'] = avg['HelpfulnessNumerator'] / avg['HelpfulnessDenominator'] Replace NAN value with 0 1avg[&quot;HelpfulRatio&quot;] = avg[&quot;HelpfulRatio&quot;].replace(np.nan, 0) 1avg .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ProductId HelpfulnessNumerator HelpfulnessDenominator Score HelpfulRatio 0 0006641040 3.027027 3.378378 4.351351 0.896000 1 141278509X 1.000000 1.000000 5.000000 1.000000 2 2734888454 0.500000 0.500000 3.500000 1.000000 3 2841233731 0.000000 0.000000 5.000000 0.000000 4 7310172001 0.809249 1.219653 4.751445 0.663507 ... ... ... ... ... ... 74253 B009UOFTUI 0.000000 0.000000 1.000000 0.000000 74254 B009UOFU20 0.000000 0.000000 1.000000 0.000000 74255 B009UUS05I 0.000000 0.000000 5.000000 0.000000 74256 B009WSNWC4 0.000000 0.000000 5.000000 0.000000 74257 B009WVB40S 0.000000 0.000000 5.000000 0.000000 74258 rows × 5 columns 1dfi.export(avg.head(10), 'img/clean_data.png') objc[5257]: Class WebSwapCGLLayer is implemented in both /System/Library/Frameworks/WebKit.framework/Versions/A/Frameworks/WebCore.framework/Versions/A/Frameworks/libANGLE-shared.dylib (0x7ffb59f48ec8) and /Applications/Google Chrome.app/Contents/Frameworks/Google Chrome Framework.framework/Versions/109.0.5414.119/Libraries/libGLESv2.dylib (0x111ded880). One of the two will be used. Which one is undefined. [0206/204727.828076:INFO:headless_shell.cc(223)] 60469 bytes written to file /var/folders/0t/vj81lwzn36xcslx3y2f148t40000gn/T/tmp9j9u523o/temp.png 3.4 Outlier detectionPart of visualization (outlier) code reference from: https://medium.com/swlh/identify-outliers-with-pandas-statsmodels-and-seaborn-2766103bf67c Method 1: Detect outlier based on Histograms 1ax = sns.distplot(avg.Score, hist=True, hist_kws={&quot;edgecolor&quot;: 'w', &quot;linewidth&quot;: 3}, kde_kws={&quot;linewidth&quot;: 3}) /var/folders/0t/vj81lwzn36xcslx3y2f148t40000gn/T/ipykernel_3210/869401958.py:1: UserWarning: `distplot` is a deprecated function and will be removed in seaborn v0.14.0. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). For a guide to updating your code to use the new functions, please see https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751 ax = sns.distplot(avg.Score, hist=True, hist_kws={&quot;edgecolor&quot;: 'w', &quot;linewidth&quot;: 3}, kde_kws={&quot;linewidth&quot;: 3}) 1ax.figure.savefig('img/dist_plot.png') Method 2: Detect outlier based on Distribution 1ax = sns.boxplot(avg.Score) 1ax.set(title='Review score box plot') [Text(0.5, 1.0, 'Review score box plot')] 1ax.figure.savefig('img/box_plot_score.png') Based on graph results above, score below 2.0 can be possible outlier, however as it’s shown from second graph, the dot is super dense for those “possible outliers”, in this case, no need to remove outlier at this point. 3.5 Other visualization1fig = sns.relplot(data = avg, x = 'HelpfulnessDenominator', y = 'HelpfulnessNumerator').set(title='Helpfulness ration among review') 1fig.savefig('img/relation_plot.png') 1avg.head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ProductId HelpfulnessNumerator HelpfulnessDenominator Score HelpfulRatio 0 0006641040 3.027027 3.378378 4 0.896000 1 141278509X 1.000000 1.000000 5 1.000000 2 2734888454 0.500000 0.500000 3 1.000000 3 2841233731 0.000000 0.000000 5 0.000000 4 7310172001 0.809249 1.219653 4 0.663507 1fig = sns.residplot(x='HelpfulnessNumerator', y='HelpfulnessDenominator', data=avg, scatter_kws=dict(s=50)) 1fig.figure.savefig('img/residual_plot.png') 1ax = plt.scatter(x= avg.index, y=avg['HelpfulRatio'], color = 'g', s= 0.5) Based on distribution graph, majority of Helpful review ratio concentrate between (0.6, 1) 1ax.set_xlabel('index') 1ax.set_ylabel('ratio') 1ax.title='Review score box plot' 1ax.figure.savefig('img/Helpful_Ratio_distribution.png') 1plt.show() Here comes the graph shows the Helpfulness numberator and Score realtion, generally, most of review get average helpfulness count regardless of score 1sns.relplot(data = avg, x = 'Score', y = 'HelpfulnessNumerator', color = 'purple') &lt;seaborn.axisgrid.FacetGrid at 0x7f7b6b9c9960&gt; 1avg_int = avg 1avg_int['Score'] = avg['Score'].astype(int) make review score five category, plot distribution of differenct helpfulness indictaors in next 3 graphs 1sns.catplot(data = avg_int, x = 'Score', y= 'HelpfulnessNumerator', kind = 'bar') &lt;seaborn.axisgrid.FacetGrid at 0x7f7ad889e200&gt; 1sns.catplot(data = avg_int, x = 'Score', y = 'HelpfulnessDenominator', kind = 'box') &lt;seaborn.axisgrid.FacetGrid at 0x7f7bbd0bd150&gt; 1sns.catplot(data = avg_int, x = 'Score', y = 'HelpfulRatio', kind = 'violin') &lt;seaborn.axisgrid.FacetGrid at 0x7f7babee9690&gt; Raw Data VS Clean DataRaw Data 1df.head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Id ProductId UserId ProfileName HelpfulnessNumerator HelpfulnessDenominator Score Time Summary Text 0 1 B001E4KFG0 A3SGXH7AUHU8GW delmartian 1 1 5 1303862400 Good Quality Dog Food I have bought several of the Vitality canned d... 1 2 B00813GRG4 A1D87F6ZCVE5NK dll pa 0 0 1 1346976000 Not as Advertised Product arrived labeled as Jumbo Salted Peanut... 2 3 B000LQOCH0 ABXLMWJIXXAIN Natalia Corres \"Natalia Corres\" 1 1 4 1219017600 \"Delight\" says it all This is a confection that has been around a fe... 3 4 B000UA0QIQ A395BORC6FGVXV Karl 3 3 2 1307923200 Cough Medicine If you are looking for the secret ingredient i... 4 5 B006K2ZZ7K A1UQRSCLF8GW1T Michael D. Bigham \"M. Wassir\" 0 0 5 1350777600 Great taffy Great taffy at a great price. There was a wid... Clean Data (Manily For quantify analysis purpose) 1avg.head(10) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ProductId HelpfulnessNumerator HelpfulnessDenominator Score HelpfulRatio 0 0006641040 3.027027 3.378378 4 0.896000 1 141278509X 1.000000 1.000000 5 1.000000 2 2734888454 0.500000 0.500000 3 1.000000 3 2841233731 0.000000 0.000000 5 0.000000 4 7310172001 0.809249 1.219653 4 0.663507 5 7310172101 0.809249 1.219653 4 0.663507 6 7800648702 0.000000 0.000000 4 0.000000 7 9376674501 0.000000 0.000000 5 0.000000 8 B00002N8SM 0.473684 0.868421 1 0.545455 9 B00002NCJC 0.000000 0.000000 4 0.000000","link":"/2023/02/05/data-prepare/"},{"title":"","text":"12import requestsimport pandas as pd Using the amazon price api from rapidapi.com 1url = &quot;https://amazon-price1.p.rapidapi.com/search&quot; Here, I am querying a bottom bracket used for replacing on my bicycle 12345678910querystring = {&quot;keywords&quot;:&quot;bottom bracket&quot;,&quot;marketplace&quot;:&quot;ES&quot;}headers = { &quot;X-RapidAPI-Key&quot;: &quot;hidden here&quot;, &quot;X-RapidAPI-Host&quot;: &quot;amazon-price1.p.rapidapi.com&quot;}response = requests.request(&quot;GET&quot;, url, headers=headers, params=querystring)print(response.text) Store the result in panda format 1df = pd.read_json(response.text) 1df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ASIN title price listPrice imageUrl detailPageURL rating totalReviews subtitle isPrimeEligible 0 B005L83TF4 SUN RACE BBS15 Bottom Bracket 68/127MM-STEEL E... 13,14 € https://m.media-amazon.com/images/I/316upP-m6I... https://www.amazon.es/dp/B005L83TF4 3.8 33 0 1 B006RM70JY The Bottom Bracket (English Edition) 2,99 € https://m.media-amazon.com/images/I/411tM3tvfN... https://www.amazon.es/dp/B006RM70JY 4.5 2 0 2 B075GQJFL1 Bottom Bracket 0,99 € https://m.media-amazon.com/images/I/71jDB9f3AK... https://www.amazon.es/dp/B075GQJFL1 0 3 B075DW91K2 Bottom Bracket 0,99 € https://m.media-amazon.com/images/I/71Tr4XpGvd... https://www.amazon.es/dp/B075DW91K2 0 4 B09D8S31V7 Bottom Bracket Racket 1,29 € https://m.media-amazon.com/images/I/41qG7oY5Ky... https://www.amazon.es/dp/B09D8S31V7 0 5 B08K2WHFS2 Dandy in the Underworld 1,29 € https://m.media-amazon.com/images/I/51UcKxQdUn... https://www.amazon.es/dp/B08K2WHFS2 0 6 B08K2VYJFK Dandy in the Underworld 1,29 € https://m.media-amazon.com/images/I/51UcKxQdUn... https://www.amazon.es/dp/B08K2VYJFK 0 7 B01J5083XQ Solstice 0,99 € https://m.media-amazon.com/images/I/51avvd1duQ... https://www.amazon.es/dp/B01J5083XQ 0 8 B01J5082CS Solstice 0,99 € https://m.media-amazon.com/images/I/51avvd1duQ... https://www.amazon.es/dp/B01J5082CS 0 9 B09J781T8V Soporte Inferior, Bottom Bracket, Token Ninja ... 74,01 € https://m.media-amazon.com/images/I/312Gc5vg36... https://www.amazon.es/dp/B09J781T8V 5.0 1 0","link":"/2023/02/05/product_api/"},{"title":"","text":"2.2 download from Kaggle1234567import osimport jsonimport pandas as pdimport numpy as npimport dataframe_image as dfiimport seaborn as snsimport matplotlib.pyplot as plt Loading the data downloaded from kaggle 1df = pd.read_csv('amazon_fine_food/Reviews.csv') 1df.head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Id ProductId UserId ProfileName HelpfulnessNumerator HelpfulnessDenominator Score Time Summary Text 0 1 B001E4KFG0 A3SGXH7AUHU8GW delmartian 1 1 5 1303862400 Good Quality Dog Food I have bought several of the Vitality canned d... 1 2 B00813GRG4 A1D87F6ZCVE5NK dll pa 0 0 1 1346976000 Not as Advertised Product arrived labeled as Jumbo Salted Peanut... 2 3 B000LQOCH0 ABXLMWJIXXAIN Natalia Corres \"Natalia Corres\" 1 1 4 1219017600 \"Delight\" says it all This is a confection that has been around a fe... 3 4 B000UA0QIQ A395BORC6FGVXV Karl 3 3 2 1307923200 Cough Medicine If you are looking for the secret ingredient i... 4 5 B006K2ZZ7K A1UQRSCLF8GW1T Michael D. Bigham \"M. Wassir\" 0 0 5 1350777600 Great taffy Great taffy at a great price. There was a wid... Export raw data as image 1dfi.export(df.head(10), 'img/raw_data.png') 3. Data cleaning and visualize3.1 Clean irrelevant data columnUserId, profileName, Timestamp is not related to product recommendation, so it’s better to remove them off 1df = df.drop('UserId', axis = 1) 1df = df.drop('ProfileName', axis = 1) 1df = df.drop('Time', axis = 1) 1df.head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Id ProductId HelpfulnessNumerator HelpfulnessDenominator Score Summary Text 0 1 B001E4KFG0 1 1 5 Good Quality Dog Food I have bought several of the Vitality canned d... 1 2 B00813GRG4 0 0 1 Not as Advertised Product arrived labeled as Jumbo Salted Peanut... 2 3 B000LQOCH0 1 1 4 \"Delight\" says it all This is a confection that has been around a fe... 3 4 B000UA0QIQ 3 3 2 Cough Medicine If you are looking for the secret ingredient i... 4 5 B006K2ZZ7K 0 0 5 Great taffy Great taffy at a great price. There was a wid... 3.2 Inspect Duplicate Data1productId_c = df.iloc[:,1:2] 1productId_c.head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ProductId count 0 B001E4KFG0 0 1 B00813GRG4 0 2 B000LQOCH0 0 3 B000UA0QIQ 0 4 B006K2ZZ7K 0 1productId_c.insert(1, 'count',0) Group by prouduct id to count num of duplicated for each id 1p_f = productId_c.groupby(['ProductId']).transform('count') 1p_f.sort_values(by=['count'], ascending=False).head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } count frequency 563881 913 913 563615 913 913 563629 913 913 563628 913 913 563627 913 913 1p_f['frequency'] = 0 1p_f.head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } count frequency 0 1 30408 1 1 30408 2 1 30408 3 1 30408 4 4 17296 1p_a = p_f.groupby(['count']).count() Group by count got early to calculate frequency for each duplicate number 1p_a['frequency'] = p_f.groupby(['count']).transform('count') 1p_a.iloc[:,0] count 1 30408 2 24524 3 20547 4 17296 5 15525 ... 564 5076 567 567 623 623 632 2528 913 913 Name: frequency, Length: 283, dtype: int64 1p_a.index Int64Index([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, ... 491, 506, 530, 542, 556, 564, 567, 623, 632, 913], dtype='int64', name='count', length=283) 1p_a.head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } frequency count 1 30408 2 24524 3 20547 4 17296 5 15525 Build an image showing the num of duplcated product existed in data set, it’s tremendous 123456789# Horizontal Bar Plot show duplicated count - num of productsplt.bar(p_a.index, p_a.iloc[0])plt.xlabel(&quot;duplicated count&quot;)plt.ylabel(&quot;num of products&quot;)plt.title(&quot;duplicated products ditribution&quot;)plt.savefig('img/duplicated_product_distribution.png')# Show Plotplt.show() 3.3 Merge Duplicated DataThe main focus here is merging duplicated data, making each product id unique For those product contains multiple scores, counting an average probably be a good choice for it. The drawback is some unique fields also need to be removed (‘Text’, ‘Summary’). Since the score plays a determined factor in recommendation, removing is necessary for certain analysis. 1df_avg = df.iloc[:, 1:5] 1df_avg .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ProductId HelpfulnessNumerator HelpfulnessDenominator Score 0 B001E4KFG0 1 1 5 1 B00813GRG4 0 0 1 2 B000LQOCH0 1 1 4 3 B000UA0QIQ 3 3 2 4 B006K2ZZ7K 0 0 5 ... ... ... ... ... 568449 B001EO7N10 0 0 5 568450 B003S1WTCU 0 0 2 568451 B004I613EE 2 2 5 568452 B004I613EE 1 1 5 568453 B001LR2CU2 0 0 5 568454 rows × 4 columns 1avg = df_avg.groupby(['ProductId']).mean() 1avg.insert(0, 'ProductId', avg.index) 1avg.index = range(len(avg.index)) Cleaned Data generated, duplicated value eliminated 1avg .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ProductId HelpfulnessNumerator HelpfulnessDenominator Score 0 0006641040 3.027027 3.378378 4.351351 1 141278509X 1.000000 1.000000 5.000000 2 2734888454 0.500000 0.500000 3.500000 3 2841233731 0.000000 0.000000 5.000000 4 7310172001 0.809249 1.219653 4.751445 ... ... ... ... ... 74253 B009UOFTUI 0.000000 0.000000 1.000000 74254 B009UOFU20 0.000000 0.000000 1.000000 74255 B009UUS05I 0.000000 0.000000 5.000000 74256 B009WSNWC4 0.000000 0.000000 5.000000 74257 B009WVB40S 0.000000 0.000000 5.000000 74258 rows × 4 columns 1avg['HelpfulRatio'] = avg['HelpfulnessNumerator'] / avg['HelpfulnessDenominator'] Replace NAN value with 0 1avg[&quot;HelpfulRatio&quot;] = avg[&quot;HelpfulRatio&quot;].replace(np.nan, 0) 1avg .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ProductId HelpfulnessNumerator HelpfulnessDenominator Score HelpfulRatio 0 0006641040 3.027027 3.378378 4.351351 0.896000 1 141278509X 1.000000 1.000000 5.000000 1.000000 2 2734888454 0.500000 0.500000 3.500000 1.000000 3 2841233731 0.000000 0.000000 5.000000 0.000000 4 7310172001 0.809249 1.219653 4.751445 0.663507 ... ... ... ... ... ... 74253 B009UOFTUI 0.000000 0.000000 1.000000 0.000000 74254 B009UOFU20 0.000000 0.000000 1.000000 0.000000 74255 B009UUS05I 0.000000 0.000000 5.000000 0.000000 74256 B009WSNWC4 0.000000 0.000000 5.000000 0.000000 74257 B009WVB40S 0.000000 0.000000 5.000000 0.000000 74258 rows × 5 columns 1dfi.export(avg.head(10), 'img/clean_data.png') objc[5257]: Class WebSwapCGLLayer is implemented in both /System/Library/Frameworks/WebKit.framework/Versions/A/Frameworks/WebCore.framework/Versions/A/Frameworks/libANGLE-shared.dylib (0x7ffb59f48ec8) and /Applications/Google Chrome.app/Contents/Frameworks/Google Chrome Framework.framework/Versions/109.0.5414.119/Libraries/libGLESv2.dylib (0x111ded880). One of the two will be used. Which one is undefined. [0206/204727.828076:INFO:headless_shell.cc(223)] 60469 bytes written to file /var/folders/0t/vj81lwzn36xcslx3y2f148t40000gn/T/tmp9j9u523o/temp.png 3.4 Outlier detectionPart of visualization (outlier) code reference from: https://medium.com/swlh/identify-outliers-with-pandas-statsmodels-and-seaborn-2766103bf67c Method 1: Detect outlier based on Histograms 1ax = sns.distplot(avg.Score, hist=True, hist_kws={&quot;edgecolor&quot;: 'w', &quot;linewidth&quot;: 3}, kde_kws={&quot;linewidth&quot;: 3}) /var/folders/0t/vj81lwzn36xcslx3y2f148t40000gn/T/ipykernel_3210/869401958.py:1: UserWarning: `distplot` is a deprecated function and will be removed in seaborn v0.14.0. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). For a guide to updating your code to use the new functions, please see https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751 ax = sns.distplot(avg.Score, hist=True, hist_kws={&quot;edgecolor&quot;: 'w', &quot;linewidth&quot;: 3}, kde_kws={&quot;linewidth&quot;: 3}) 1ax.figure.savefig('img/dist_plot.png') Method 2: Detect outlier based on Distribution 1ax = sns.boxplot(avg.Score) 1ax.set(title='Review score box plot') [Text(0.5, 1.0, 'Review score box plot')] 1ax.figure.savefig('img/box_plot_score.png') Based on graph results above, score below 2.0 can be possible outlier, however as it’s shown from second graph, the dot is super dense for those “possible outliers”, in this case, no need to remove outlier at this point. 3.5 Other visualization1fig = sns.relplot(data = avg, x = 'HelpfulnessDenominator', y = 'HelpfulnessNumerator').set(title='Helpfulness ration among review') 1fig.savefig('img/relation_plot.png') 1avg.head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ProductId HelpfulnessNumerator HelpfulnessDenominator Score HelpfulRatio 0 0006641040 3.027027 3.378378 4 0.896000 1 141278509X 1.000000 1.000000 5 1.000000 2 2734888454 0.500000 0.500000 3 1.000000 3 2841233731 0.000000 0.000000 5 0.000000 4 7310172001 0.809249 1.219653 4 0.663507 1fig = sns.residplot(x='HelpfulnessNumerator', y='HelpfulnessDenominator', data=avg, scatter_kws=dict(s=50)) 1fig.figure.savefig('img/residual_plot.png') 1ax = plt.scatter(x= avg.index, y=avg['HelpfulRatio'], color = 'g', s= 0.5) Based on distribution graph, majority of Helpful review ratio concentrate between (0.6, 1) 1ax.set_xlabel('index') 1ax.set_ylabel('ratio') 1ax.title='Review score box plot' 1ax.figure.savefig('img/Helpful_Ratio_distribution.png') 1plt.show() Here comes the graph shows the Helpfulness numberator and Score realtion, generally, most of review get average helpfulness count regardless of score 1sns.relplot(data = avg, x = 'Score', y = 'HelpfulnessNumerator', color = 'purple') &lt;seaborn.axisgrid.FacetGrid at 0x7f7b6b9c9960&gt; 1avg_int = avg 1avg_int['Score'] = avg['Score'].astype(int) make review score five category, plot distribution of differenct helpfulness indictaors in next 3 graphs 1sns.catplot(data = avg_int, x = 'Score', y= 'HelpfulnessNumerator', kind = 'bar') &lt;seaborn.axisgrid.FacetGrid at 0x7f7ad889e200&gt; 1sns.catplot(data = avg_int, x = 'Score', y = 'HelpfulnessDenominator', kind = 'box') &lt;seaborn.axisgrid.FacetGrid at 0x7f7bbd0bd150&gt; 1sns.catplot(data = avg_int, x = 'Score', y = 'HelpfulRatio', kind = 'violin') &lt;seaborn.axisgrid.FacetGrid at 0x7f7babee9690&gt; Raw Data VS Clean DataRaw Data 1df.head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Id ProductId UserId ProfileName HelpfulnessNumerator HelpfulnessDenominator Score Time Summary Text 0 1 B001E4KFG0 A3SGXH7AUHU8GW delmartian 1 1 5 1303862400 Good Quality Dog Food I have bought several of the Vitality canned d... 1 2 B00813GRG4 A1D87F6ZCVE5NK dll pa 0 0 1 1346976000 Not as Advertised Product arrived labeled as Jumbo Salted Peanut... 2 3 B000LQOCH0 ABXLMWJIXXAIN Natalia Corres \"Natalia Corres\" 1 1 4 1219017600 \"Delight\" says it all This is a confection that has been around a fe... 3 4 B000UA0QIQ A395BORC6FGVXV Karl 3 3 2 1307923200 Cough Medicine If you are looking for the secret ingredient i... 4 5 B006K2ZZ7K A1UQRSCLF8GW1T Michael D. Bigham \"M. Wassir\" 0 0 5 1350777600 Great taffy Great taffy at a great price. There was a wid... Clean Data (Manily For quantify analysis purpose) 1avg.head(10) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ProductId HelpfulnessNumerator HelpfulnessDenominator Score HelpfulRatio 0 0006641040 3.027027 3.378378 4 0.896000 1 141278509X 1.000000 1.000000 5 1.000000 2 2734888454 0.500000 0.500000 3 1.000000 3 2841233731 0.000000 0.000000 5 0.000000 4 7310172001 0.809249 1.219653 4 0.663507 5 7310172101 0.809249 1.219653 4 0.663507 6 7800648702 0.000000 0.000000 4 0.000000 7 9376674501 0.000000 0.000000 5 0.000000 8 B00002N8SM 0.473684 0.868421 1 0.545455 9 B00002NCJC 0.000000 0.000000 4 0.000000","link":"/2023/02/06/Data_kaggle_clean/Data_kaggle_clean/"},{"title":"Clustering","text":"1. PrefaceNowadays, people live in material word with so many choices overwhelming to the daily life, sometimes, finding an item simple as salt would be time-consuming when walking around a big supermarket. Categorize plays a big role to help us limit the search scope, making our searching more easily and quality. @Source: https://www.cleanpng.com/png-cluster-analysis-spectral-clustering-k-means-clust-1466403/download-png.html 2. OverviewThe main object here is clustering / categorizing the best sell amazon book during 2009 ~ 2019, based on year, reviews, prices and other attributes, researchers can find sort of similarity between these books in different styles, customers can have a view of group type and find similar book based on their favourite. @Source: image from paper Tom Tullis, Bill Albert, in Measuring the User Experience (Second Edition), 2013 Partitional ClusteringImplemented in python, using standard K-means alogrithm for the process, pick a couple of K choices, and use silhouette method and repeat experiment to ensure the clustering quality. Euclidean distance used. Hierarchical ClusteringImplemented in R, using complete linkage clustering, generating the dendodiagram, and playing around different parameter seting to find a good clustering seperation. Cosine Similarity distance used. 3. Data PrepFor clustering data column, the dataset comes with user rating, number of reviews ,prices, years (publish) as original numerical data type. Besides, ‘Genere_n’ column (filled with value 0/1) will be created based on string type column genere, which indicates whether the book belongs to fiction, will also be used as clustering parameter. Finally, z-score normalization will be applied to some columns.Detail for Data prepartion (Python):https://github.com/BraydenZheng/Product_Recommendation/blob/master/clustering/data_prepare.ipynb Link to sample data: book_clustering 4. CodeCode for K-means (Python): https://github.com/BraydenZheng/Product_Recommendation/blob/master/clustering/kmeans.ipynb Code for Hierarchical Clustering (R): https://braydenzheng.github.io/clustering/skip_render/hierarchical.html 5. Result5.1 K-MeansAt the begining, I tried K = 3 for 5 dimension, althogh 5-D is hard to visualize, I still choose two feature (‘User rating’ and ‘Price’) to visualize the cluster. From picture above, we can see points dense in the center with overlap (500+ points in the graph), but still a clear outline for each cluster. To further discover the best K, I used Silhouette score to try k value range between 3 and 21, and got the graph below. Looks like when K equal to around 3, we have best silhouette score, which also shows in the graph with highest score around 0.41. When it goes to 5 or 6, it still have silhouette score around 0.33 while maintaing a good number of clusters, which is a great balance between cluster number and silhouette score. To visualize and decide the best K, I also plot the cluster image for K = 4, 5, 6. (I put 5 features for clustering, in order for plot 2-D, I only choose two features) While it’s hard to determine the best K based on these 2-D graphs, and the three graphs do have dense points gathered in the middle, we still can find the border between each one especially for the outer part, becasue these 3 choices all have good silhouetee score and distribution, I will choose K equal to 6 for a cluster of total 550 books, which will be just on the average so that customer will not feel overwhlemdw with too many categories but also maintains a good variety. 5.2 Hierchical ClusteringSimilar setting is also used for hierarchical clustering, after applying cosine similarity as dist, I tried K as [5, 6, 7] for the clustering, and get graph results below. K = 5K = 6K = 7From visualization above, one common thing I observe is one cluster took majority of the space (Blue for K == 5, 6 and Red for K == 7). Regards to best K choice, I considered factors that make the tree balanced, having a suitable number of clusters that not too high or too low, also letting each cluster has similar size (keep variance low). K as 7 in this case did best in these valuation factor, which has a good representation of each category also balanced size of the tree. Compare to K - means, the best K for two algorithms is pretty close (K-means is 6, hclust is 7). For choice in K-means, the silhouette coefficient plays an important role so I choose a small K which makes data points close to each other. For hclust, I consdier more about the tree balanced and its portion ‘big picture’, both of them lead to a similar result at the end. ConclusionsIt’s good practice to apply these two efficient methods for the book clustering, I learned different factors and consideration for find a best K value, that’s also find the number of category for the book recommendation actually. Sometimes, all these values, scores (Silhouette coefficient and tree height), even for distruibtion give us a reference for choosing the K - category, but make a final decison for both K and clustering usually requries extra consideration for business factors or the customer preference.","link":"/2023/02/24/Clustering/"},{"title":"ARM","text":"1. OverviewARM is usually used to find relationships between different elements occured in the dataset. Here, I choose a online retail dataset as the datasource, and brieftly describe the common term used in arm. Support refers to the frequency of both antecedent and consequent products occur in dataset. Confidence refer to given the antecedent product, the probability that customer buy the consequent product. @image source: https://www.softwaretestinghelp.com/apriori-algorithm/ Lift is the ratio of support of union antecedent and consequent item to the product of single support of antecent and single support of consequent item. Rule is an expression of relation between antecedent and consequent items. In the retail dataset, the rule describe the relation that a customer buy certain product based on buy other products. Apriori algorithm is used to find the association rules given by certain support, confidence, lift and other conditions. The Apriori will start with finding the frequent buying single product in retail example, then based on preset minimum support, confidence, the algorithm only keeps those products satisfy the conditons, and use these products to generate new products sets, it will keep repeating the loop until all rules under conditions found. @image source: https://imgbin.com/download/tFwH2qpi 2. Data PrepRaw data from Kaggle (Online Retail Transaction): https://www.kaggle.com/datasets/mathchi/online-retail-data-set-from-ml-repositoryResult transaction data: Generally, I transferred row based transaction data to column based, removed the unused columns, and tested a couple of times for the suitable amount of data, make sure the dataset is not too large to casue out of memory error and not too less which makes graph lack of data point, finally I did some transformation to make it ‘transaction data’ format in R. Detailed process (Python):https://github.com/BraydenZheng/Product_Recommendation/blob/master/arm/retail_data_clean.ipynb 3. Code ARM (R)https://braydenzheng.github.io/arm/skip_render/arm.html 4. ResultHere I set 0.1% support and 0.2 confidence for apriori algorithm argument, with 766911 rules generated, and most of rulse contains one or two items in both antecedent and consequent, product buying relation is low in general, but still many connection between buying behaviour. **Top 15 rules for lift:** For the top 15 lift rule, we can see all of these consist of single item, which makes sense for transaction in online retail, single combination (eg. bread + milk) will be most common compared with multiple itemest. Also, the lift value is around 15, we can see the strong connection between antecedent and consequent items there. **Top 15 rules for confidence:**The layout for top 15 confidence rules looks scattered in the plot graph, but actually they are condense with same difference in the value, most high confidence rules has support over 0.06 which is quite high in these dataset with only 200 transcation picked. **Top 15 rules for support:**The heighest support went to 0.2 with lift around 6, I do see some grey dot on rightdown corner, with high support but low lift, even it’s single item relation rule, compared with other two graphs above, look like the support is least influcence factor for the strength of association. 5. ConclusionFrom the practice and observation above, I see the ‘support’, ‘confidence’, ‘lift’ all play differnet roles in association measurement, depend on the scenario, we may look into ‘support’ for finding the buying association with huge orders base, consider ‘lift’ for the buy together items with strong connection, and finally use these rules to help with the product promption and recommendation.","link":"/2023/02/28/ARM/"},{"title":"NaiveBayes","text":"1. OverviewA Naive Bayes classifier is a probabilistic machine learning model used for classification, the most famous implementation is email classification to identify the potential spam. Multinomial NB algorithm is variant of Naive Bayes, it assumes features are independt, and can be applied to multiple output labels for single model. Bernoulli is also an naive bayes algorithm mostly used for binary classifcation, and the input features must be binary variable as well (eg. True, False), each feature is considered independent. Here, we will use Standard Multinomial NB to classify the amazon saling data, and predit an expected rating based on given features. @Image Source: https://charanhu.medium.com/naive-bayes-algorithm-2a9415e21034 2. Data PrepHere I use amazon store sales data from kaggle as the raw data.It’s easy to find the original dataset contains lots of text data columns and inconsistent format between numerical columns. I did following steps to clean optimize the data. Keep only numerical value as input feature, for navie training purpose Discretize ‘rating’ column, and take it as label, the continuous label output didn’t work well for NaiveBayes model. Clean and format Numerical data Normailze data with min-max scaler Code Step: https://github.com/BraydenZheng/Product_Recommendation/blob/master/naivebayes/data_prepare.ipynb Split training and testing data as 80%, 20% portion accordingly The purpose of creating a disjoint split is to ensure that model only testing on unseen data during evaluation, which closely resembles real-world scenarios. Cleaned Data 3. CodeModel training and evaluation: https://github.com/BraydenZheng/Product_Recommendation/blob/master/naivebayes/naivebayes.ipynb 4. ResultAccuracy: 0.78 After several rounds of tuning, the model achieve the accuracy 78% on test data. The majority of data has discrete rating as ‘3’ or ‘4’, while model has good performance overall, one weakness of this model is lacking of the ability to predict label rating ‘3’, actullay that’s even not shown on predicted test data. 5. Conclusion Normalization matters, I got 60% accuracy for data without normalization, the accuracy rise to 78% after applying min-max scaler Take caution when using MNB for continuous output data. To apply MNB on continuous label, we need to first discretize data label and then feed into the model. However, discretize data sometimes cause the output label too concentrated, which leads to the weakness for model predit on margin data.","link":"/2023/03/12/NaiveBayes/"},{"title":"DecisonTree","text":"1. OverviewDecison Tree is popular ML algorithm can be applied for both regression and classification problems. The decision tree contains structure called ‘rule’ and ‘node’, where ‘rule’ is the judging condition and leaf is the decison result. image source: https://www.researchgate.net/figure/A-simple-example-of-a-decision-tree-for-the-classification-of-emails-The-geometric_fig2_265554646 Typical applications includes email spam filter, product classification, and even stock market prediction. GINI, Entropy, and Information Gain are all used to assist deciding the best rule to split data at each level. They are all useful tools to reduce the impurity of data node, usually we maximize the information gain to make decison tree more effective and use less depth. Here is an example using GINI and information gain. Assume we have 10 fruits in total, 7 as apples and 3 as banana.Before applying any rule, the GINI is: 1 - (P(A)^2 + P(B)^2) = 1 - ((7/10)^2 + (3/10)^2) = 0.42 Then we apply rule 1, for example: weight &gt; 0.2lbs, then we got:left node: 5 apples and 1 bananaright node: 2 apples and 2 banana GINI left = 1 - ((5/6)^2 + (1/6)^2) = 0.28GINI right = 1 - ((1/2)^2 + (1/2)^2) = 0.5 Information gain = 0.42 - (6/10 * 0.28 + 4/10 * 0.5) = 0.05 The information gain 0.05 is a good indicator for how good the split is. **Why it is generally possible to create an infinite number of trees?**Because there are so many different paramater we can specify for decison tree, such as MaxDepth, GINI or Entropy, also there are many ways for feature representation, each combination above can create a unique tree. 2. Data PrepSame cleaned sales dataset and testing / training spit method as NaiveBayes model training, however one difference is the data normalization steps are removed, since it’s not required and can be confusing when we visualizing the tree. Code Step: https://github.com/BraydenZheng/Product_Recommendation/blob/master/naivebayes/data_prepare.ipynb Raw Data Cleaned Data","link":"/2023/03/24/DecisonTree/"}],"tags":[{"name":"dataprep","slug":"dataprep","link":"/tags/dataprep/"},{"name":"clustering","slug":"clustering","link":"/tags/clustering/"},{"name":"arm","slug":"arm","link":"/tags/arm/"},{"name":"naivebayes","slug":"naivebayes","link":"/tags/naivebayes/"},{"name":"dectrees","slug":"dectrees","link":"/tags/dectrees/"}],"categories":[],"pages":[{"title":"Introduction","text":"Amazon.com is world wide e-commerce retailer that sells a huge range of products online. Many ranking show amazon is second biggest retailer in US in 2022, how can amazon become so successful? People all know amazon brings superior customer service, with blazing fast delivery and risk free return option, customer service is always amazon’s first priority. But what else make amazon so different besides its service? There are a couple of differenct perspectives here. Recommendation system and ads definitely plays an important part there. In 2015, an experiment was conducted that performing recommendation A visual dataset of styles and substitutes in amazon store. (https://cseweb.ucsd.edu/~jmcauley/pdfs/sigir15.pdf). In 2016, Ruining and Julian developed models to Modeling the Visual Evolution of Fashion Trends with One-Class Collaborative Filtering. (https://cseweb.ucsd.edu/~jmcauley/pdfs/www16a.pdf). Attach below is a sample review crawlered from amazon website, showing related product info, customer’s rating, review text details. Clustering and ARM are common ways to categorize data and find the strengh of the relationship between different products. Clustering in a retail store can be helpful for helping customers find products efficiently, also help to boost the store saling. While ARM can tell the common products customer buy together, so that store manager can put these stuffs in same place, and kindly remind the consumer to bring them at the same time. However, these stragey looks simple, to apply and make good use of them usually takes time and efforts. \\ 10 questions to answer about Retail How to gather retail data How to process the missing / incorrect data How to do data normalization How to find the production relation How to classify the product based on category How to cluster the products for saleing purpose How to personally recommend the product to the user How to apply svm to retail How to find common trend in transcation How to label the product for saling purpose","link":"/Introduction/index.html"},{"title":"Clustering","text":"","link":"/Clustering/index.html"},{"title":"","text":"hierarchical.knit // Pandoc 2.9 adds attributes on both header and div. We remove the former (to // be compatible with the behavior of Pandoc < 2.8). document.addEventListener('DOMContentLoaded', function(e) { var hs = document.querySelectorAll(\"div.section[class*='level'] > :first-child\"); var i, h, a; for (i = 0; i < hs.length; i++) { h = hs[i]; if (!/^h[1-6]$/i.test(h.tagName)) continue; // it should be a header h1-h6 a = h.attributes; while (a.length > 0) h.removeAttribute(a[0].name); } }); /*! jQuery v3.6.0 | (c) OpenJS Foundation and other contributors | jquery.org/license */ !function(e,t){\"use strict\";\"object\"==typeof module&&\"object\"==typeof module.exports?module.exports=e.document?t(e,!0):function(e){if(!e.document)throw new Error(\"jQuery requires a window with a document\");return t(e)}:t(e)}(\"undefined\"!=typeof window?window:this,function(C,e){\"use strict\";var t=[],r=Object.getPrototypeOf,s=t.slice,g=t.flat?function(e){return t.flat.call(e)}:function(e){return t.concat.apply([],e)},u=t.push,i=t.indexOf,n={},o=n.toString,v=n.hasOwnProperty,a=v.toString,l=a.call(Object),y={},m=function(e){return\"function\"==typeof e&&\"number\"!=typeof e.nodeType&&\"function\"!=typeof e.item},x=function(e){return null!=e&&e===e.window},E=C.document,c={type:!0,src:!0,nonce:!0,noModule:!0};function b(e,t,n){var r,i,o=(n=n||E).createElement(\"script\");if(o.text=e,t)for(r in c)(i=t[r]||t.getAttribute&&t.getAttribute(r))&&o.setAttribute(r,i);n.head.appendChild(o).parentNode.removeChild(o)}function w(e){return null==e?e+\"\":\"object\"==typeof e||\"function\"==typeof e?n[o.call(e)]||\"object\":typeof e}var f=\"3.6.0\",S=function(e,t){return new S.fn.init(e,t)};function p(e){var t=!!e&&\"length\"in e&&e.length,n=w(e);return!m(e)&&!x(e)&&(\"array\"===n||0===t||\"number\"==typeof t&&0","link":"/Clustering/skip_render/hierarchical.html"},{"title":"arm","text":"","link":"/arm/index.html"},{"title":"","text":"ARM // Pandoc 2.9 adds attributes on both header and div. We remove the former (to // be compatible with the behavior of Pandoc < 2.8). document.addEventListener('DOMContentLoaded', function(e) { var hs = document.querySelectorAll(\"div.section[class*='level'] > :first-child\"); var i, h, a; for (i = 0; i < hs.length; i++) { h = hs[i]; if (!/^h[1-6]$/i.test(h.tagName)) continue; // it should be a header h1-h6 a = h.attributes; while (a.length > 0) h.removeAttribute(a[0].name); } }); /*! jQuery v3.6.0 | (c) OpenJS Foundation and other contributors | jquery.org/license */ !function(e,t){\"use strict\";\"object\"==typeof module&&\"object\"==typeof module.exports?module.exports=e.document?t(e,!0):function(e){if(!e.document)throw new Error(\"jQuery requires a window with a document\");return t(e)}:t(e)}(\"undefined\"!=typeof window?window:this,function(C,e){\"use strict\";var t=[],r=Object.getPrototypeOf,s=t.slice,g=t.flat?function(e){return t.flat.call(e)}:function(e){return t.concat.apply([],e)},u=t.push,i=t.indexOf,n={},o=n.toString,v=n.hasOwnProperty,a=v.toString,l=a.call(Object),y={},m=function(e){return\"function\"==typeof e&&\"number\"!=typeof e.nodeType&&\"function\"!=typeof e.item},x=function(e){return null!=e&&e===e.window},E=C.document,c={type:!0,src:!0,nonce:!0,noModule:!0};function b(e,t,n){var r,i,o=(n=n||E).createElement(\"script\");if(o.text=e,t)for(r in c)(i=t[r]||t.getAttribute&&t.getAttribute(r))&&o.setAttribute(r,i);n.head.appendChild(o).parentNode.removeChild(o)}function w(e){return null==e?e+\"\":\"object\"==typeof e||\"function\"==typeof e?n[o.call(e)]||\"object\":typeof e}var f=\"3.6.0\",S=function(e,t){return new S.fn.init(e,t)};function p(e){var t=!!e&&\"length\"in e&&e.length,n=w(e);return!m(e)&&!x(e)&&(\"array\"===n||0===t||\"number\"==typeof t&&0","link":"/arm/skip_render/arm.html"},{"title":"DecisonTree","text":"","link":"/DecisonTree/index.html"},{"title":"NaiveBayes","text":"","link":"/NaiveBayes/index.html"}]}